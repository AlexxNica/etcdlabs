<md-sidenav-layout>
    <md-sidenav #sidenav [opened]="true" mode="side" class="doc-sidenav">
        <md-list dense>
            <a routerLink="/doc/{{version.etcdVersionURL}}">
                <h3 md-subheader>Getting Started ({{version.etcdVersion}})</h3>
            </a>
            <md-list-item *ngFor="let item of getStartedItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>

            <md-divider></md-divider>
            <h3 md-subheader>Operation Guides</h3>
            <md-list-item *ngFor="let item of operationItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>
        </md-list>
    </md-sidenav>

    <div class="doc-group">
        <router-outlet></router-outlet>

        <h4>Disaster Recovery</h4>
        <p class="narrow-paragraph">
            etcd is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to (N-1)/2 permanent failures for a cluster of N members. When a member permanently fails, whether
            due to hardware failure or disk corruption, it loses access to the cluster. If the cluster permanently loses more than (N-1)/2 members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus
            and therefore cannot continue accepting updates.
        </p>
        <p class="narrow-paragraph">
            To recover from disastrous failure, etcd v3 provides snapshot and restore facilities to recreate the cluster without v3 key data loss.
        </p>


        <br>
        <h5>Snapshotting the keyspace</h5>
        <p class="narrow-paragraph">
            Recovering a cluster first needs a snapshot of the keyspace from an etcd member. A snapshot may either be taken from a live member with the <span class="code-light-snippet">etcdctl snapshot save</span> command or by copying the <span class="code-light-snippet">$DATA_DIR/member/snap/db</span>            file from an etcd data directory. For example, the following command snapshots the keyspace served by <span class="code-light-snippet">$ENDPOINT</span> to the file <span class="code-light-snippet">backup.db</span>:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">$ ETCDCTL_API=3 etcdctl --endpoints=$ENDPOINTS snapshot save backup.db
Snapshot saved at backup.db


$ ETCDCTL_API=3 etcdctl --write-out=table snapshot status backup.db
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| e5d0e1c6 |       12 |          8 | 2.1 MB     |
+----------+----------+------------+------------+
</pre>
                </div>
            </div>
        </div>


        <br>
        <h5>Restoring a cluster</h5>
        <p class="narrow-paragraph">
            To restore a cluster, all that is needed is a single snapshot <span class="code-light-snippet">db</span> file. A cluster restore with <span class="code-light-snippet">etcdctl snapshot restore</span> creates new etcd data directories; all members
            should restore using the same snapshot. Restoring overwrites some snapshot metadata (specifically, the member ID and cluster ID); the member loses its former identity. This metadata overwrite prevents the new member from inadvertently joining
            an existing cluster. Therefore in order to start a cluster from a snapshot, the restore must start a new logical cluster.
        </p>
        <p class="narrow-paragraph">
            Snapshot integrity may be optionally verified at restore time. If the snapshot is taken with <span class="code-light-snippet">etcdctl snapshot save</span>, it will have an integrity hash that is checked by <span class="code-light-snippet">etcdctl snapshot restore</span>.
            If the snapshot is copied from the data directory, there is no integrity hash and it will only restore by using <span class="code-light-snippet">--skip-hash-check</span>.
        </p>
        <p class="narrow-paragraph">
            A restore initializes a new member of a new cluster, with a fresh cluster configuration using etcd's cluster configuration flags, but preserves the contents of the etcd keyspace. Continuing from the previous example, the following creates new etcd data
            directories (m1.etcd, m2.etcd, m3.etcd) for a three member cluster:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">$ ETCDCTL_API=3 etcdctl snapshot restore backup.db \
  --name m1 \
  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host1:2380

$ ETCDCTL_API=3 etcdctl snapshot restore backup.db \
  --name m2 \
  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host2:2380

$ ETCDCTL_API=3 etcdctl snapshot restore backup.db \
  --name m3 \
  --initial-cluster m1=http:/host1:2380,m2=http://host2:2380,m3=http://host3:2380 \
  --initial-cluster-token etcd-cluster-1 \
  --initial-advertise-peer-urls http://host3:2380
</pre>
                </div>
            </div>
        </div>
        <p class="narrow-paragraph">
            Next, start etcd with the new data directories:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">$ etcd \
  --name m1 \
  --listen-client-urls http://host1:2379 \
  --advertise-client-urls http://host1:2379 \
  --listen-peer-urls http://host1:2380

$ etcd \
  --name m2 \
  --listen-client-urls http://host2:2379 \
  --advertise-client-urls http://host2:2379 \
  --listen-peer-urls http://host2:2380

$ etcd \
  --name m3 \
  --listen-client-urls http://host3:2379 \
  --advertise-client-urls http://host3:2379 \
  --listen-peer-urls http://host3:2380
</pre>
                </div>
            </div>
        </div>
        <p class="narrow-paragraph">
            Now the restored etcd cluster should be available and serving the keyspace given by the snapshot.
        </p>

    </div>
</md-sidenav-layout>