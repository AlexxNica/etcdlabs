<md-sidenav-layout>
    <md-sidenav #sidenav [opened]="true" mode="side" class="doc-sidenav">
        <md-list>
            <a routerLink="/doc/{{version.etcdVersionURL}}">
                <h3 md-subheader>Getting Started ({{version.etcdVersion}})</h3>
            </a>
            <md-list-item *ngFor="let item of getStartedItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>

            <md-divider></md-divider>

            <h3 md-subheader>More</h3>
            <md-list-item *ngFor="let item of moreItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>
        </md-list>
    </md-sidenav>

    <div class="doc-group">
        <router-outlet></router-outlet>

        <div class="inner">
            <div class="block block-copy half">
                <div class="faq-list-title">Distributed System</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#consensus-algorithm" class="faq-list">Consensus algorithm</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#cap-theorem" class="faq-list">CAP theorem</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#consistency" class="faq-list">Consistency</a>
                <br>
            </div>
            <div class="block block-copy half">
                <div class="faq-list-title">Membership</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#remove-member-first" class="faq-list">Always remove first when replacing member?</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#why-so-strict-about-membership-change" class="faq-list">Why so strict about membershp change?</a>
                <br>
            </div>
        </div>

        <br>
        <hr>
        <br>

        <div id="consensus-algorithm"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#consensus-algorithm" class="faq-title">Consensus algorithm</a></h2>
        <p>
            etcd uses Raft<sup>[1]</sup> consensus algorithm. It can handle faulty processes in distributed computing systems and keep data consistent even when system loses one of its communications. Raft ensures safety, which means system never present
            incorrect values under non-Byzantine failures, such as network partitions and delays. System is fully functional as long as any majority of servers can communicate with each other. Command completes as soon as quorum has responded, and minority
            does not affect the overall performance.
        </p>
        <hr align="left" class="footer-top-line">
        <footer>
            [1] Diego Ongaro and John K Ousterhout: "<a href="https://raft.github.io/raft.pdf" target="_blank" class="footer-link">In Search of an Understandable Consensus Algorithm (Extended Version)</a>," at USENIX Annual Technical Conference (ATC),
            June 2014.
        </footer>
        <br><br>

        <div id="cap-theorem"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#cap-theorem" class="faq-title">CAP theorem</a></h2>
        <p>
            CAP<sup>[1]</sup> represents <i>Consistency</i>, <i>Availability</i>, <i>Partition-tolerance</i>: you can only pick 2 out of 3, it is impossible that a distributed computer system simultaneously satisfies them all. Since network partition
            is not avoidable, you are left with either consitency or availability when partition happens. That is, systems with <i>Availability</i> and <i>Partition-tolerance</i> are more tolerant of network faults, but possible to serve stale data. etcd
            chooses <i>Consistency</i> and <i>Partition-tolerance</i> to achieve sequential consistency<sup>[2]</sup>.
        </p>
        <hr align="left" class="footer-top-line">
        <footer>
            [1] Seth Gilbert and Nancy Lynch: "<a href="https://pdfs.semanticscholar.org/24ce/ce61e2128780072bc58f90b8ba47f624bc27.pdf" target="_blank" class="footer-link">Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services</a>,"
            ACM SIGACT News, volume 33, number 2, pages 51–59, 2002.
            <br> [2] Leslie Lamport: "<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/multi.pdf" target="_blank" class="footer-link">How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs</a>," IEEE Trans.
            Comput. C-28,9, pages 690-691, September 1979.
        </footer>
        <br><br>

        <div id="consistency"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#consistency" class="faq-title">Consistency</a></h2>
        <p>
            Linearizability<sup>[1]</sup> (also known as <i>strong, atomic consistency</i>) is an important requirement for implementing distributed locks. The basic idea is to make distributed nodes work like a single copy, with all operations being
            atomic. etcd provides two types of read consistency: linearizable <i>or quorum</i> read that requires majority to agree on the value before returning to client. Serializable read is returned from local node without going through consensus
            protocol. Serializable read normally has higher throughput but possibly serving stale data, while linearizable read returns the latest value written.
        </p>
        <p>
            In etcd v3.0, quorum read throughput is around 40,000 requests per second (QPS), while serializable read is over 100,000 QPS (2x faster). Quorum read is slower because it needs to issue a read query command, and this command gets replicated, which costs
            disk I/O. etcd v3.1 implements <i>read index</i> mechanism described in Raft<sup>[2]</sup> paper. etcd quorum read is now as fast as serializable read: Leader records its current committed index in
            <span class="code-light-snippet">readIndex</span> field. Leader sends <span class="code-light-snippet">readIndex</span> to its followers. For followers, <span class="code-light-snippet">readIndex</span> is the largest committed index ever
            seen by any server because it is from leader and committed. Then read queries as far as <span class="code-light-snippet">readIndex</span> are now served locally with linearizability. This is more efficient because it avoids synchronous disk
            writes still preserving the linearizability.
        </p>
        <p>
            etcd is not an eventually consistent database, where two different nodes can serve two different values (stale data). etcd achieves <b>sequential consistency</b><sup>[3]</sup>, where ordering of commands is preserverd for all clients. Some
            claim that Dynamo-style<sup>[4]</sup> databases (e.g., Riak and Cassandra) can also achieve strong or sequential consistency by requiring quorum for reads and writes. This is not true. Last-write-wins<sup>[5]</sup> conflict resolution is not
            linearizable when clock skew happens. Multiple concurrent writes to the same key may arrive in different order, different nodes. All etcd writes are replicated through its leader to followers. Entry is <i>committed</i> when replicated in majority
            of cluster. Each read returns the value at the latest committed write. etcd provides two types of read consistency.
        </p>
        <hr align="left" class="footer-top-line">
        <footer>
            [1] Maurice P Herlihy and Jeannette M Wing: "<a href="http://www.cs.cornell.edu/andru/cs711/2002fa/reading/linearizability.pdf" target="_blank" class="footer-link">Linearizability: A Correctness Condition for Concurrent Objects</a>," ACM Transactions
            on Programming Languages and Systems (TOPLAS), volume 12, number 3, pages 463–492, July 1990.
            <br> [2] Diego Ongaro: "<a href="https://ramcloud.stanford.edu/~ongaro/thesis.pdf" target="_blank" class="footer-link">Consensus: Bridging Theory and Practice</a>," Stanford University Ph.D. Dissertation, chapter 6, page 72, August 2014.
            <br> [3] Hagit Attiya and Jennifer L Welch: "<a href="http://dl.acm.org/citation.cfm?id=176576" target="_blank" class="footer-link">Sequential Consistency versus Linearizability</a>," ACM Transactions on Computer Systems (TOCS), volume 12,
            number 2, pages 91–122, May 1994.
            <br> [4] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: "<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" class="footer-link">Dynamo: Amazon's Highly Available Key-Value Store</a>,"
            at 21st ACM Symposium on Operating Sys‐ tems Principles (SOSP), October 2007.
            <br> [5] Jonathan Ellis: "<a href="http://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks" target="_blank" class="footer-link">Why Cassandra doesn’t need vector clocks</a>," datastax.com, 2 September 2013.
        </footer>
        <br><br>

        <div id="remove-member-first"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#remove-member-first" class="faq-title">Always remove first when replacing member?</a></h2>
        <p>
            When replacing an etcd node, we recommend to remove the member first and then add its replacement<sup>[1]</sup>. etcd employs distributed consensus based on a quorum model; (n+1)/2 members, a majority, must agree on a proposal before it can
            be committed to the cluster. These proposals include key-value updates and membership changes. This model totally avoids any possibility of split brain inconsistency. The downside is permanent quorum loss is catastrophic.
        </p>
        <p>
            How this applies to membership:
        </p>
        <p>
            If a 3-member cluster has 1 downed member, it can still make forward progress because the quorum is 2 and 2 members are still live. However, adding a new member to a 3-member cluster will increase the quorum to 3 because 3 votes are required for a majority
            of 4 members. Since the quorum increased, this extra member buys nothing in terms of fault tolerance; the cluster is still one node failure away from being unrecoverable.
        </p>
        <p>
            Additionally, that new member is risky because it may turn out to be misconfigured or incapable of joining the cluster. In that case, there's no way to recover quorum because the cluster has two members down and two members up, but needs three votes to
            change membership to undo the botched membership addition. etcd will by default (as of last week) reject member add attempts that could take down the cluster in this manner.
        </p>
        <p>
            On the other hand, if the downed member is removed from cluster membership first, the number of members becomes 2 and the quorum remains at 2. Following that removal by adding a new member will also keep the quorum steady at 2. So, even if the new node
            can't be brought up, it's still possible to remove the new member through quorum on the remaining live members.
        </p>
        <hr align="left" class="footer-top-line">
        <footer>
            [1] "<a href="https://github.com/coreos/etcd/issues/6114" target="_blank" class="footer-link">Adding replacement member before removing</a>," etcd GitHub issue.
        </footer>
        <br><br>

        <div id="why-so-strict-about-membership-change"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#why-so-strict-about-membership-change" class="faq-title">Why so strict about membershp change?</a></h2>
        <p>
            etcd sets <span class="code-light-snippet">strict-reconfig-check</span> in order to reject reconfiguration requests that would cause quorum loss. Abandoning quorum is really risky (especially when the cluster is already in a bad way)<sup>[1]</sup>.
            We're aware that losing quorum is painful, but disabling quorum on membership could lead to full fledged cluster inconsistency and that would be even worse in many applications ("disk geometry corruption" being a candidate for most terrifying).
            It's too dangerous to be a legitimate fix, sorry. Permitting a member add when the cluster is unhealthy is clearly broken and the fix for that, which is safe, is already inflight.
        </p>
        <hr align="left" class="footer-top-line">
        <footer>
            [1] "<a href="https://github.com/coreos/etcd/issues/6103" target="_blank" class="footer-link">Can't add or remove node in unhealthy cluster</a>," etcd GitHub issue.
        </footer>
        <br><br>
    </div>
</md-sidenav-layout>