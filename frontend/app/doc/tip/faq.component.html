<md-sidenav-layout>
    <md-sidenav #sidenav [opened]="true" mode="side" class="doc-sidenav">
        <md-list dense>
            <a routerLink="/doc/{{version.etcdVersionURL}}">
                <h3 md-subheader>Getting Started ({{version.etcdVersion}})</h3>
            </a>
            <md-list-item *ngFor="let item of getStartedItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>

            <md-divider></md-divider>
            <h3 md-subheader>Operation Guides</h3>
            <md-list-item *ngFor="let item of operationItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>
        </md-list>
    </md-sidenav>

    <div class="doc-group">
        <router-outlet></router-outlet>

        <div id="top"></div>

        <div class="inner">
            <div class="block block-copy half">

                <div class="faq-list-title">etcd, Distributed Systems</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd" class="faq-list">What is etcd?</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-to-use" class="faq-list">When to use etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-not-to-use" class="faq-list">When not to use etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-consensus" class="faq-list">Consensus protocol in etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-cap" class="faq-list">CAP theorem in etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-consistency" class="faq-list">Consistency model in etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#follower-leader" class="faq-list">Follower, leader in etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-failure-tolerance" class="faq-list">Failure tolerance in etcd</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-use-case-coreos" class="faq-list">Who uses etcd: CoreOS</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcd-use-case-kubernetes" class="faq-list">Who uses etcd: Kubernetes</a>
                <br>

            </div>
            <div class="block block-copy half">

                <div class="faq-list-title">Operation, Membership</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#odd-cluster-size" class="faq-list">Why odd number of cluster size?</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#max-cluster-size" class="faq-list">What is maximum cluster size?</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#best-deployment-practice" class="faq-list">Best deployment practice</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#remove-member-first" class="faq-list">Always remove first when replacing member?</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#why-so-strict-about-membership-change" class="faq-list">Why so strict about membershp change?</a>
                <br>

                <br>
                <div class="faq-list-title">Flags</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#flag-client-urls" class="faq-list">listen-client-urls vs. advertise-client-urls</a>
                <br>

                <br>
                <div class="faq-list-title">Errors, Warning</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#apply-too-long-unavailable" class="faq-list">apply entries took too long, unavailable or misconfigured</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#request-cluster-id-mismatch" class="faq-list">request cluster ID mismatch</a>
                <br>

                <br>
                <div class="faq-list-title">Others</div>
                <a href="/doc/{{version.etcdVersionURL}}/faq#other-questions" class="faq-list">Any other questions?</a>
                <br>
                <a href="/doc/{{version.etcdVersionURL}}/faq#etcdlabs" class="faq-list">What is this website?</a>
                <br>

            </div>
        </div>

        <br>
        <hr>
        <br>

        <div id="etcd"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd" class="faq-title">What is etcd?</a></h2>
        <p class="narrow-paragraph">
            etcd is distributed, transactional key-value store. It is designed to store the most critical data of distributed systems. Similar to how Linux uses <span class="code-light-snippet">/etc</span> directory to store local configurations, etcd
            is a reliable store for distributed configuration data. Data is replicated to multiple etcd nodes, therefore highly available against single point of failures. Using the Raft<sup>[1]</sup> consensus algorithm, etcd gracefully handles network
            partitions and machine failures, even leader failures. etcd is inspired by Google's Chubby lock service<sup>[2]</sup>, and completely <a href="https://github.com/coreos/etcd" target="_blank" class="normal-link">open-source</a>.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Diego Ongaro and John K Ousterhout: "<a href="https://raft.github.io/raft.pdf" target="_blank" class="footer-link">In Search of an Understandable Consensus Algorithm (Extended Version)</a>," at USENIX Annual Technical Conference (ATC),
            June 2014.
            <br> [2] Mike Burrows: "<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf" target="_blank" class="footer-link">The Chubby lock service for loosely-coupled distributed systems</a>," at 7th
            USENIX Symposium on Operating System Design and Implementation (OSDI), November 2006.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="etcd-to-use"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-to-use" class="faq-title">When to use etcd</a></h2>
        <p class="narrow-paragraph">
            etcd is designed to store a small amount of meta-data in consistent, fault-tolerant way. Distributed systems use etcd as the root of scheduling and service discovery configuration storage. <a href="https://godoc.org/github.com/coreos/etcd/clientv3/concurrency"
                target="_blank" class="normal-link">Distributed lock</a> is implemented on top of etcd, useful for master election in other distributed databases.
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

        <div id="etcd-not-to-use"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-not-to-use" class="faq-title">When not to use etcd</a></h2>
        <p class="narrow-paragraph">
            etcd can only handle small chunks data: configuration file, JSON, YAML, etc. etcd limits the size of request in 1.5MB (see <a href="https://github.com/coreos/etcd/blob/master/etcdserver/v3_server.go#L34-L38" target="_blank" class="code-link">maxRequestBytes</a>).
            And default storage size limit is 2GB (see <a href="https://github.com/coreos/etcd/blob/master/mvcc/backend/backend.go#L46-L53" target="_blank" class="code-link">DefaultQuotaBytes</a>). These are the tradeoffs that etcd makes for strong consistency.
            etcd is not a relational SQL database. etcd uses <i>key</i> as the primary index: clients can range- and prefix-scan on the keys. etcd does not have a secondary index.
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

        <div id="etcd-consensus"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-consensus" class="faq-title">Consensus protocol in etcd</a></h2>
        <p class="narrow-paragraph">
            etcd uses Raft<sup>[1]</sup> consensus algorithm. It keeps data consistent even if a cluster has faulty process or loses one of its communications. Raft safety property guarantees that system never returns incorrect value under network partitions
            and delays (<i>non-Byzantine failures</i>). Raft is fully functional as long as any majority of servers can communicate with each other. Command completes as soon as quorum responds; overall performance is not affected by the minority.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Diego Ongaro and John K Ousterhout: "<a href="https://raft.github.io/raft.pdf" target="_blank" class="footer-link">In Search of an Understandable Consensus Algorithm (Extended Version)</a>," at USENIX Annual Technical Conference (ATC),
            June 2014.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="etcd-cap"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-cap" class="faq-title">CAP theorem in etcd</a></h2>
        <p class="narrow-paragraph">
            CAP<sup>[1]</sup> represents <i>Consistency</i>, <i>Availability</i>, <i>Partition-tolerance</i>: you can only pick 2 out of 3, it is impossible that a distributed computer system simultaneously satisfies them all. Since network partition
            is not avoidable, you are left with either consitency or availability when partition happens. System with <i>availability</i> and <i>partition-tolerance</i> is more tolerant of network faults, but possible to serve stale data. etcd chooses
            <i>consistency</i> and <i>partition-tolerance</i>.
        </p>
        <section class="features narrow-paragraph">
            <div class="inner">
                <div class="feature-grid">
                    <div class="table-row">
                        <div class="feature">
                            <h4>Consistent</h4>
                            <br>
                            <p>
                                etcd is not an eventually consistent database, where two different nodes can serve two different values (stale data). etcd provides <b>linearizable writes and reads</b><sup>[2]</sup> with <b>sequential consistency</b><sup>[3]</sup>.
                                When write completes, all etcd clients would read the same, <i>most recent and up-to-date</i>, value.
                            </p>
                        </div>

                        <div class="feature">
                            <h4>Partition Tolerant</h4>
                            <br>
                            <p>
                                etcd continues to operate under transient network faults, where message is not delivered or gets delayed. Network glitches are very common in practice, and <b>no system is immune</b> from network faults<sup>[4]</sup>.
                            </p>
                        </div>

                        <div class="feature">
                            <h4>Highly Available</h4>
                            <br>
                            <p>
                                etcd is <i>highly</i> available, choosing <b>consistency</b> over availability when <b>network partitioned</b>. etcd can make progress as long as majority of servers are available. For example, 5-node etcd cluster tolerates
                                up to 2 nodes being down, as long as 3 are up.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Seth Gilbert and Nancy Lynch: "<a href="https://pdfs.semanticscholar.org/24ce/ce61e2128780072bc58f90b8ba47f624bc27.pdf" target="_blank" class="footer-link">Brewer's Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services</a>,"
            ACM SIGACT News, volume 33, number 2, pages 51–59, 2002.
            <br> [2] Maurice P Herlihy and Jeannette M Wing: "<a href="http://www.cs.cornell.edu/andru/cs711/2002fa/reading/linearizability.pdf" target="_blank" class="footer-link">Linearizability: A Correctness Condition for Concurrent Objects</a>,"
            ACM Transactions on Programming Languages and Systems (TOPLAS), volume 12, number 3, pages 463–492, July 1990.
            <br> [3] Hagit Attiya and Jennifer L Welch: "<a href="http://dl.acm.org/citation.cfm?id=176576" target="_blank" class="footer-link">Sequential Consistency versus Linearizability</a>," ACM Transactions on Computer Systems (TOCS), volume 12,
            number 2, pages 91–122, May 1994.
            <br> [4] Peter Bailis and Kyle Kingsbury: "<a href="http://queue.acm.org/detail.cfm?id=2655736" target="_blank" class="footer-link">The Network is Reliable</a>," ACM Queue, volume 12, number 7, July 2014.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="etcd-consistency"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-consistency" class="faq-title">Consistency model in etcd</a></h2>
        <p class="narrow-paragraph">
            Linearizability<sup>[1]</sup> (also known as strong, atomic consistency) is an important requirement for implementing distributed locks. The basic idea is to make distributed nodes work like a single copy, with all operations being atomic.
            etcd provides two types of read consistency: linearizable (or <i>quorum</i>) read that requires majority to agree on the value before responding to client. Serializable read is returned from local node without going through consensus protocol.
            Serializable read normally has higher throughput but possibly serving stale data, whereas linearizable read always returns the latest value written.
        </p>
        <p class="narrow-paragraph">
            In etcd v3.0, quorum read throughput is around 40,000 requests per second (QPS), whereas serializable read is over 100,000 QPS (2x faster). Quorum read is slower because it needs to issue a read query command, and this command gets replicated, costs disk
            I/O. <b>etcd v3.1+</b> implements <i>read index</i> mechanism<sup>[2]</sup>. <b>etcd quorum read is now as fast as serializable read</b>: Leader records its current committed index in <span class="code-light-snippet">readIndex</span>            field. Leader sends <span class="code-light-snippet">readIndex</span> to its followers. For followers, <span class="code-light-snippet">readIndex</span> is the largest committed index ever seen by any server because it is from leader and committed.
            Then read queries as far as <span class="code-light-snippet">readIndex</span> can be served locally without talking to other replicas, but still with linearizability. This is more efficient because it avoids synchronous disk writes and still
            preserves the linearizability.
        </p>
        <p class="narrow-paragraph">
            etcd is not an eventually consistent database, where two different nodes can serve two different values (stale data). <b>etcd achieves sequential consistency</b><sup>[3]</sup>, where ordering of commands is preserverd for all clients. Some
            claim that Dynamo-style<sup>[4]</sup> databases (e.g., Riak and Cassandra) can also achieve strong or sequential consistency by requiring quorum for reads and writes. This is not true. Last-write-wins<sup>[5]</sup> conflict resolution is not
            linearizable when clock skew happens. Multiple concurrent writes to the same key may arrive in different order, different nodes. Committed transactions can be lost by the later writes before the record gets replicated (i.e., not durable).
        </p>
        <p class="narrow-paragraph">
            All etcd writes go to leader, then fowarded to followers. Entry is <i>committed</i> when replicated in majority of cluster. Clients won't be able to read that entry until it is committed, and each command is applied in the same order.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Maurice P Herlihy and Jeannette M Wing: "<a href="http://www.cs.cornell.edu/andru/cs711/2002fa/reading/linearizability.pdf" target="_blank" class="footer-link">Linearizability: A Correctness Condition for Concurrent Objects</a>," ACM Transactions
            on Programming Languages and Systems (TOPLAS), volume 12, number 3, pages 463–492, July 1990.
            <br> [2] Diego Ongaro: "<a href="https://ramcloud.stanford.edu/~ongaro/thesis.pdf" target="_blank" class="footer-link">Consensus: Bridging Theory and Practice</a>," Stanford University Ph.D. Dissertation, chapter 6, page 72, August 2014.
            <br> [3] Hagit Attiya and Jennifer L Welch: "<a href="http://dl.acm.org/citation.cfm?id=176576" target="_blank" class="footer-link">Sequential Consistency versus Linearizability</a>," ACM Transactions on Computer Systems (TOCS), volume 12,
            number 2, pages 91–122, May 1994.
            <br> [4] Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, et al.: "<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" class="footer-link">Dynamo: Amazon's Highly Available Key-Value Store</a>,"
            at 21st ACM Symposium on Operating Sys‐ tems Principles (SOSP), October 2007.
            <br> [5] Jonathan Ellis: "<a href="http://www.datastax.com/dev/blog/why-cassandra-doesnt-need-vector-clocks" target="_blank" class="footer-link">Why Cassandra doesn’t need vector clocks</a>," datastax.com, 2 September 2013.
        </footer>
        <br><br>

        <div id="follower-leader"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#follower-leader" class="faq-title">Follower, leader in etcd</a></h2>
        <p class="narrow-paragraph">
            Raft is leader-based, and the leader handles all client requests. Client does not need to know which node is the leader. Request to follower is automatically forwarded to leader. Raft algorithm divides time into terms, and each term begins with an election
            to choose one leader. If leader becomes unavailable, cluster immediately elects a new leader or automatically transfers the leadership to a follower. This requires no human intervention.
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

        <div id="etcd-failure-tolerance"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-failure-tolerance" class="faq-title">Failure tolerance in etcd</a></h2>
        <p class="narrow-paragraph">
            etcd cluster continues to operate as long as quorum of cluster can communicate with each other. etcd automatically handles transient failures (e.g. network partitions and delays) while ensuring the safety properties in Raft<sup>[1]</sup>.
            etcd can also survive hardware failures (e.g. power outages) by persisting Write-Ahead-Log files onto disk, which contain all stream of events that happened. User just needs to restart the node with same configuration, and etcd will automatically
            replay its log to the exact point of time before failure.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Diego Ongaro: "<a href="https://ramcloud.stanford.edu/~ongaro/thesis.pdf" target="_blank" class="footer-link">Consensus: Bridging Theory and Practice</a>," Stanford University Ph.D. Dissertation, chapter 3, page 22, August 2014.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="etcd-use-case-coreos"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-use-case-coreos" class="faq-title">Who uses etcd: CoreOS</a></h2>
        <p class="narrow-paragraph">
            Application running on <a href="https://coreos.com/why/" target="_blank" class="normal-link">CoreOS</a> gets automatic, no-downtime Linux kernel updates. CoreOS uses
            <a href="https://github.com/coreos/locksmith" target="_blank" class="normal-link">locksmith</a> to coordinate updates. locksmith stores semephore values in etcd to ensure that only subset of cluster are rebooting at any given time.
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

        <div id="etcd-use-case-kubernetes"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcd-use-case-kubernetes" class="faq-title">Who uses etcd: Kubernetes</a></h2>
        <p class="narrow-paragraph">
            <a href="http://kubernetes.io/docs/whatisk8s/" target="_blank" class="normal-link">Kubernetes</a> needs configuration storage for service discovery and cluster management. Kubernetes API server writes persistent cluster states in etcd. And
            uses etcd watch API to get notified whenever changes happen. Consistency is the key to ensure that services correctly schedule and operatate.
        </p>
        <br>

        <div id="odd-cluster-size"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#odd-cluster-size" class="faq-title">Why odd number of cluster size?</a></h2>
        <p class="narrow-paragraph">
            A quorum is the majority of nodes in a cluster. When <span class="code-light-snippet">n</span> is the cluster size, quorum is <span class="code-light-snippet">(n/2)+1</span>. 2 is the quorum both for <span class="code-light-snippet">3</span>            and <span class="code-light-snippet">4</span>. Since Raft cluster becomes unavailable if the quorum of nodes is unavailable, adding another node to 3-node cluster does not add any value in terms of fault tolerance. And adding 1 node to single-node
            cluster increases the quorum size and causes leader election; if a wrong node were added by mistake, the cluster would lose its leader and not be able to make any progress. Always start with an odd number of nodes.
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

        <div id="max-cluster-size"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#max-cluster-size" class="faq-title">What is maximum cluster size?</a></h2>
        <p class="narrow-paragraph">
            Theorectially, there is no hard limit. However, we recommend no more than 7 nodes. The more nodes cluster has, the more communication needs to happen, which would reduce overall performance. Google Chubby lock service<sup>[1]</sup>, which
            is predecessor of etcd, typically runs 5 nodes in each cell.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Mike Burrows: "<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf" target="_blank" class="footer-link">The Chubby lock service for loosely-coupled distributed systems</a>," at 7th USENIX
            Symposium on Operating System Design and Implementation (OSDI), chapter 2, page 3, November 2006.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="best-deployment-practice"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#best-deployment-practice" class="faq-title">Best deployment practice</a></h2>
        <p class="narrow-paragraph">
            It highly depends on production use case. 3 or 5 is the recommended cluster size, as Google Chubby does<sup>[1]</sup>. And optionally
            <a href="https://github.com/coreos/etcd/blob/master/etcdctl/doc/mirror_maker.md" target="_blank"><span class="code-light-snippet">mirror</span></a> the cluster to another data center. Cluster can also be distributed across multiple data centers.
            This would require time parameter tuning to handle higher latency.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] Mike Burrows: "<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf" target="_blank" class="footer-link">The Chubby lock service for loosely-coupled distributed systems</a>," at 7th USENIX
            Symposium on Operating System Design and Implementation (OSDI), November 2006.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="remove-member-first"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#remove-member-first" class="faq-title">Always remove first when replacing member?</a></h2>
        <p class="narrow-paragraph">
            When replacing an etcd node, we recommend to remove the member first and then add its replacement<sup>[1]</sup>. etcd employs distributed consensus based on a quorum model; (n+1)/2 members, a majority, must agree on a proposal before it can
            be committed to the cluster. These proposals include key-value updates and membership changes. This model totally avoids any possibility of split brain inconsistency. The downside is permanent quorum loss is catastrophic.
        </p>
        <p class="narrow-paragraph">
            How this applies to membership: If a 3-member cluster has 1 downed member, it can still make forward progress because the quorum is 2 and 2 members are still live. However, adding a new member to a 3-member cluster will increase the quorum to 3 because
            3 votes are required for a majority of 4 members. Since the quorum increased, this extra member buys nothing in terms of fault tolerance; the cluster is still one node failure away from being unrecoverable.
        </p>
        <p class="narrow-paragraph">
            Additionally, that new member is risky because it may turn out to be misconfigured or incapable of joining the cluster. In that case, there's no way to recover quorum because the cluster has two members down and two members up, but needs three votes to
            change membership to undo the botched membership addition. etcd will by default (as of last week) reject member add attempts that could take down the cluster in this manner.
        </p>
        <p class="narrow-paragraph">
            On the other hand, if the downed member is removed from cluster membership first, the number of members becomes 2 and the quorum remains at 2. Following that removal by adding a new member will also keep the quorum steady at 2. So, even if the new node
            can't be brought up, it's still possible to remove the new member through quorum on the remaining live members.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] "<a href="https://github.com/coreos/etcd/issues/6114" target="_blank" class="footer-link">Adding replacement member before removing</a>," etcd GitHub issue.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="why-so-strict-about-membership-change"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#why-so-strict-about-membership-change" class="faq-title">Why so strict about membershp change?</a></h2>
        <p class="narrow-paragraph">
            etcd sets <span class="code-light-snippet">strict-reconfig-check</span> in order to reject reconfiguration requests that would cause quorum loss. Abandoning quorum is really risky (especially when the cluster is already in a bad way)<sup>[1]</sup>.
            We're aware that losing quorum is painful, but disabling quorum on membership could lead to full fledged cluster inconsistency and that would be even worse in many applications ("disk geometry corruption" being a candidate for most terrifying).
            It's too dangerous to be a legitimate fix, sorry. Permitting a member add when the cluster is unhealthy is clearly broken and the fix for that, which is safe, is already inflight.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] "<a href="https://github.com/coreos/etcd/issues/6103" target="_blank" class="footer-link">Can't add or remove node in unhealthy cluster</a>," etcd GitHub issue.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="flag-client-urls"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#flag-client-urls" class="faq-title">listen-client-urls vs. advertise-client-urls</a></h2>
        <p class="narrow-paragraph">
            <span class="code-light-snippet">listen-client-urls</span> is the list of URLs to listen on for client requests: etcd clients request via these endpoints. <span class="code-light-snippet">advertise-client-urls</span> is the list of client
            URLs to advertise to other members, proxies, clients of the cluster<sup>[1]</sup>. etcd distinguishes between listen URLs and advertise URLs. A listen URL determines which interface etcd will listen on to receive new connections. The advertise
            URL is what the cluster broadcasts as the address for connecting to the cluster. For example, etcd can be configured to listen on <span class="code-light-snippet">0.0.0.0</span> (all interfaces) but advertise the IP of the machine so remote
            systems can find it<sup>[2]</sup>.
        </p>
        <p class="narrow-paragraph">
            A common mistake is setting <span class="code-light-snippet">advertise-client-urls</span> to localhost or default host. Please note that <span class="code-light-snippet">advertise-client-urls</span> is the list of endpoitns for remote clients
            or traffic. For example if you run etcd on a server with IP's <span class="code-light-snippet">1.2.3.4</span> and <span class="code-light-snippet">4.5.6.7</span> but only want to serve client requests through <span class="code-light-snippet">1.2.3.4:2379</span>,
            you'd pass <span class="code-light-snippet">--advertise-client-urls=http://1.2.3.4:2379</span> to etcd.
        </p>
        <p class="narrow-paragraph">
            Suppose etcd is listening on <span class="code-light-snippet">127.0.0.1</span> and a public IP <span class="code-light-snippet">1.2.3.4</span>. If it advertises <span class="code-light-snippet">127.0.0.1</span> and <span class="code-light-snippet">1.2.3.4</span>            then clients that synchronize their endpoints with the cluster will try to connect to the cluster through <span class="code-light-snippet">127.0.0.1</span> in addition to <span class="code-light-snippet">1.2.3.4</span>.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] "<a href="https://github.com/coreos/etcd/issues/6539" target="_blank" class="footer-link">advertise-client-urls question</a>," etcd GitHub issue.
            <br> [2] "<a href="https://github.com/coreos/etcd/pull/6583#issuecomment-251768299" target="_blank" class="footer-link">github.com/coreos/etcd/pull/6583</a>," etcd GitHub comment.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="apply-too-long-unavailable"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#apply-too-long-unavailable" class="faq-title">apply entries took too long, unavailable or misconfigured</a></h2>
        <p class="narrow-paragraph">
            Please evaluate the workload before using etcd in production. etcd is for most critical data in software stack<sup>[1]</sup>. It is highly recommended to allocate dedidate disk for etcd process. etcd does a lot of I/O to replicate Write-Ahead-Log
            and key-value data on disk. If etcd is run on slow disk (e.g. HDD or overloaded drive), it would print out warnings of
            <span class="code-light-snippet">apply entries took too long</span> or <span class="code-light-snippet">unavailable or misconfigured</span>. This indicates that the etcd cluster is overloaded, which in many cases, leads to leader election
            and halted cluster.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] "<a href="https://github.com/coreos/etcd/issues/6555" target="_blank" class="footer-link">etcdctl is not tolerant to etcd delays under heavy IO</a>," etcd GitHub issue.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>

        <div id="request-cluster-id-mismatch"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#request-cluster-id-mismatch" class="faq-title">request cluster ID mismatch</a></h2>
        <p class="narrow-paragraph">
            Each node in etcd cluster shares an unique cluster ID to reject unverified members. If a node tries to join the cluster from the old data with different clsuter ID, it will be rejected with <span class="code-light-snippet">request cluster ID mismatch</span>.
            If membership change happened, members from old cluster configuration would be rejected as well<sup>[1]</sup>.
        </p>
        <hr align="left" class="footer-top-line">
        <footer class="narrow-footer">
            [1] "<a href="https://github.com/coreos/etcd/issues/6181" target="_blank" class="footer-link">request cluster ID mismatch</a>," etcd GitHub issue.
        </footer>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br><br>


        <div id="other-questions"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#other-questions" class="faq-title">Any other questions?</a></h2>
        <p class="narrow-paragraph">
            Best way to reach us is via <a href="https://github.com/coreos/etcd/issues" target="_blank" class="normal-link">GitHub Issues</a>.
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

        <div id="etcdlabs"></div>
        <h2><a href="/doc/{{version.etcdVersionURL}}/faq#etcdlabs" class="faq-title">What is this website?</a></h2>
        <p class="narrow-paragraph">
            This website contains <a href="https://github.com/coreos/etcd" target="_blank" class="normal-link">etcd</a> documentation, tutorials, and demos. This is completely <a href="https://github.com/coreos/etcdlabs" target="_blank" class="normal-link">open-source</a>.
            Contribution is always welcomed!
        </p>
        <div align="right" class="narrow-paragraph"><a href="/doc/{{version.etcdVersionURL}}/faq#top" class="normal-link">↑ top</a></div>
        <br>

    </div>
</md-sidenav-layout>