<md-sidenav-layout>
    <md-sidenav #sidenav [opened]="true" mode="side" class="doc-sidenav">
        <md-list dense>
            <a routerLink="/doc/{{version.etcdVersionURL}}">
                <h3 md-subheader>Getting Started ({{version.etcdVersion}})</h3>
            </a>
            <md-list-item *ngFor="let item of getStartedItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>

            <md-divider></md-divider>
            <h3 md-subheader>Operation Guides</h3>
            <md-list-item *ngFor="let item of operationItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>
        </md-list>
    </md-sidenav>

    <div class="doc-group">
        <router-outlet></router-outlet>

        <h4>Runtime Reconfiguration</h4>
        <p class="narrow-paragraph">
            Runtime reconfiguration is one of the hardest and most error prone features in a distributed system, especially in a consensus based system like etcd. Read on to learn about the design of etcd's runtime reconfiguration commands and how we tackled these
            problems.
        </p>


        <br>
        <h5>Two phase config changes keep the cluster safe</h5>
        <p class="narrow-paragraph">
            In etcd, every runtime reconfiguration has to go through two phases for safety reasons. For example, to add a member, first inform cluster of new configuration and then start the new member.
        </p>
        <p class="narrow-paragraph">
            Phase 1 - Inform cluster of new configuration
        </p>
        <p class="narrow-paragraph">
            To add a member into etcd cluster, make an API call to request a new member to be added to the cluster. This is only way to add a new member into an existing cluster. The API call returns when the cluster agrees on the configuration change.
        </p>
        <p class="narrow-paragraph">
            Phase 2 - Start new member
        </p>
        <p class="narrow-paragraph">
            To join the etcd member into the existing cluster, specify the correct <span class="code-light-snippet">initial-cluster</span> and set <span class="code-light-snippet">initial-cluster-state</span> to <span class="code-light-snippet">existing</span>.
            When the member starts, it will contact the existing cluster first and verify the current cluster configuration matches the expected one specified in <span class="code-light-snippet">initial-cluster</span>. When the new member successfully
            starts, the cluster has reached the expected configuration.
        </p>
        <p class="narrow-paragraph">
            By splitting the process into two discrete phases users are forced to be explicit regarding cluster membership changes. This actually gives users more flexibility and makes things easier to reason about. For example, if there is an attempt to add a new
            member with the same ID as an existing member in an etcd cluster, the action will fail immediately during phase one without impacting the running cluster. Similar protection is provided to prevent adding new members by mistake. If a new etcd
            member attempts to join the cluster before the cluster has accepted the configuration change,, it will not be accepted by the cluster.
        </p>
        <p class="narrow-paragraph">
            Without the explicit workflow around cluster membership etcd would be vulnerable to unexpected cluster membership changes. For example, if etcd is running under an init system such as systemd, etcd would be restarted after being removed via the membership
            API, and attempt to rejoin the cluster on startup. This cycle would continue every time a member is removed via the API and systemd is set to restart etcd after failing, which is unexpected.
        </p>
        <p class="narrow-paragraph">
            We expect runtime reconfiguration to be an infrequent operation. We decided to keep it explicit and user-driven to ensure configuration safety and keep the cluster always running smoothly under explicit control.
        </p>


        <br>
        <h5>Permanent loss of quorum requires new cluster</h5>
        <p class="narrow-paragraph">
            If a cluster permanently loses a majority of its members, a new cluster will need to be started from an old data directory to recover the previous state.
        </p>
        <p class="narrow-paragraph">
            It is entirely possible to force removing the failed members from the existing cluster to recover. However, we decided not to support this method since it bypasses the normal consensus committing phase, which is unsafe. If the member to remove is not
            actually dead or force removed through different members in the same cluster, etcd will end up with a diverged cluster with same clusterID. This is very dangerous and hard to debug/fix afterwards.
        </p>
        <p class="narrow-paragraph">
            With a correct deployment, the possibility of permanent majority lose is very low. But it is a severe enough problem that worth special care. We strongly suggest reading the <a routerLink="/doc/{{version.etcdVersionURL}}/disaster-recovery"
                class="normal-link">disaster recovery</a> documentation and prepare for permanent majority lose before putting etcd into production.
        </p>


        <br>
        <h5>Do not use public discovery service for runtime reconfiguration</h5>
        <p class="narrow-paragraph">
            The public discovery service should only be used for bootstrapping a cluster. To join member into an existing cluster, use runtime reconfiguration API.
        </p>
        <p class="narrow-paragraph">
            Discovery service is designed for bootstrapping an etcd cluster in the cloud environment, when the IP addresses of all the members are not known beforehand. After successfully bootstrapping a cluster, the IP addresses of all the members are known. Technically,
            the discovery service should no longer be needed.
        </p>
        <p class="narrow-paragraph">
            It seems that using public discovery service is a convenient way to do runtime reconfiguration, after all discovery service already has all the cluster configuration information. However relying on public discovery service brings troubles:
        </p>
        <ul class="narrow-paragraph">
            <li>
                It introduces external dependencies for the entire life-cycle of the cluster, not just bootstrap time. If there is a network issue between the cluster and public discovery service, the cluster will suffer from it.
            </li>
            <li>
                Public discovery service must reflect correct runtime configuration of the cluster during it life-cycle. It has to provide security mechanism to avoid bad actions, and it is hard.
            </li>
            <li>
                Public discovery service has to keep tens of thousands of cluster configurations. Our public discovery service backend is not ready for that workload.
            </li>
        </ul>
        <p class="narrow-paragraph">
            To have a discovery service that supports runtime reconfiguration, the best choice is to build a private one.
        </p>


        <br>
        <h5>Runtime Reconfiguration</h5>
        <p class="narrow-paragraph">
            etcd comes with support for incremental runtime reconfiguration, which allows users to update the membership of the cluster at run time. Reconfiguration requests can only be processed when the majority of the cluster members are functioning. It is <b>highly recommended to always have a cluster size greater than two in production</b>.
            It is unsafe to remove a member from a two member cluster. The majority of a two member cluster is also two. If there is a failure during the removal process, the cluster might not able to make progress and need to restart from majority failure.
            Before making any change, the simple majority (quorum) of etcd members must be available. This is essentially the same requirement as for any other write to etcd.
        </p>
        <h6>Cycle or upgrade multiple machines</h6>
        <p class="narrow-paragraph">
            If multiple cluster members need to move due to planned maintenance (hardware upgrades, network downtime, etc.), it is recommended to modify members one at a time. It is safe to remove the leader, however there is a brief period of downtime while the
            election process takes place. If the cluster holds more than 50MB, it is recommended to <a href="/doc/{{version.etcdVersionURL}}/maintenance#snapshot-backup" class="normal-link">backup the member's data directory</a>.
        </p>
        <h6>Change the cluster size</h6>
        <p class="narrow-paragraph">
            Increasing the cluster size can enhance failure tolerance and provide better read performance. Since clients can read from any member, increasing the number of members increases the overall read throughput. Decreasing the cluster size can improve the
            write performance of a cluster, with a trade-off of decreased resilience. Writes into the cluster are replicated to a majority of members of the cluster before considered committed. Decreasing the cluster size lowers the majority, and each
            write is committed more quickly.
        </p>
        <h6>Replace a failed machine</h6>
        <p class="narrow-paragraph">
            If a machine fails due to hardware failure, data directory corruption, or some other fatal situation, it should be replaced as soon as possible. Machines that have failed but haven't been removed adversely affect the quorum and reduce the tolerance for
            an additional failure. To replace the machine, first remove the member from the cluster, and then add a new member in its place. If the cluster holds more than 50MB, it is recommended to <a href="/doc/{{version.etcdVersionURL}}/maintenance#snapshot-backup"
                class="normal-link">backup the member's data directory</a>.
        </p>
        <h6>Restart cluster from majority failure</h6>
        <p class="narrow-paragraph">
            If the majority of the cluster is lost or all of the nodes have changed IP addresses, then manual action is necessary to recover safely. The basic steps in the recovery process is explained <a routerLink="/doc/{{version.etcdVersionURL}}/disaster-recovery"
                class="normal-link">here</a>.
        </p>


        <br>
        <h5>Update a member</h5>
        <p class="narrow-paragraph">
            To update the advertise client URLs of a member, simply restart that member with updated <span class="code-light-snippet">--advertise-client-urls</span> or environment variable <span class="code-light-snippet">ETCD_ADVERTISE_CLIENT_URLS</span>.
            The restarted member will self publish the updated URLs. A wrongly updated client URL will not affect the health of the etcd cluster.
        </p>
        <p class="narrow-paragraph">
            To update the advertise peer URLs of a member, first update it explicitly via member command and then restart the member. The additional action is required since updating peer URLs changes the cluster wide configuration and can affect the health of the
            etcd cluster.
        </p>
        <p class="narrow-paragraph">
            To update the peer URLs, first, we need to find the target member's ID. To list all members with <span class="code-light-snippet">etcdctl member list</span>. And specify the new peer URLs with <span class="code-light-snippet">--peer-urls</span>            flag:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">$ ETCDCTL_API=3 etcdctl member list
8211f1d0f64f3269, started, infra1, http://127.0.0.1:12380, http://127.0.0.1:2379
91bc3c398fb3c146, started, infra2, http://127.0.0.1:22380, http://127.0.0.1:22379
fd422379fda50e48, started, infra3, http://127.0.0.1:32380, http://127.0.0.1:32379

$ ETCDCTL_API=3 etcdctl member update 8211f1d0f64f3269 --peer-urls=http://10.0.1.10:2380
Member 8211f1d0f64f3269 updated in cluster ef37ad9dc622a7c4
</pre>
                </div>
            </div>
        </div>

        <br>
        <h5>Remove a member</h5>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">$ ETCDCTL_API=3 etcdctl member list
8211f1d0f64f3269, started, infra1, http://127.0.0.1:12380, http://127.0.0.1:2379
91bc3c398fb3c146, started, infra2, http://127.0.0.1:22380, http://127.0.0.1:22379
fd422379fda50e48, started, infra3, http://127.0.0.1:32380, http://127.0.0.1:32379

$ ETCDCTL_API=3 etcdctl member remove 8211f1d0f64f3269
Member 8211f1d0f64f3269 removed from cluster ef37ad9dc622a7c4
</pre>
                </div>
            </div>
        </div>
        <p class="narrow-paragraph">
            The target member will stop itself at this point and print out the removal in the log:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">etcdserver: the member has been permanently removed from the cluster
etcdserver: the data-dir used by this member must be removed.
</pre>
                </div>
            </div>
        </div>
        <p class="narrow-paragraph">
            It is safe to remove the leader, however the cluster will be inactive while a new leader is elected. This duration is normally the period of election timeout plus the voting process.
        </p>


        <br>
        <h5>Add a new member</h5>
        <p class="narrow-paragraph">
            To add a new member to cluster, first run <span class="code-light-snippet">etcdctl member add</span>. And then start the new member with new <span class="code-light-snippet">--initial-cluster</span> flag and <span class="code-light-snippet">--initial-cluster-state=existing</span>.
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">$ ETCDCTL_API=3 etcdctl member add my-etcd-4 --peer-urls=http://localhost:42380
Member 8211f1d0f64f3269 added to cluster ef37ad9dc622a7c4

$ etcd --initial-cluster=NEW_INITIAL_CLUSTER --initial-cluster-state=existing
</pre>
                </div>
            </div>
        </div>
        <p class="narrow-paragraph">
            The new member will run as a part of the cluster and immediately begin catching up with the rest of the cluster. If adding multiple members the best practice is to configure a single member at a time and verify it starts correctly before adding more new
            members. If adding a new member to a 1-node cluster, the cluster cannot make progress before the new member starts because it needs two members as majority to agree on the consensus. This behavior only happens between the time
            <span class="code-light-snippet">etcdctl member add</span> informs the cluster about the new member and the new member successfully establishing a connection to the existing one.
        </p>
        <p class="narrow-paragraph">
            Make sure to delete data-dir of removed member. When a new member starts using the data directory of a removed member and connects to any active member in the cluster, etcd will exit automatically with error message <span class="code-light-snippet-red">the member has been permanently removed from the cluster</span>.
        </p>

        <br>
        <h5>Strict reconfiguration check mode</h5>
        <p class="narrow-paragraph">
            As described in the above, the best practice of adding new members is to configure a single member at a time and verify it starts correctly before adding more new members. This step by step approach is very important because if newly added members is
            not configured correctly (for example the peer URLs are incorrect), the cluster can lose quorum. The quorum loss happens since the newly added member are counted in the quorum even if that member is not reachable from other existing members.
            Also quorum loss might happen if there is a connectivity issue or there are operational issues.
        </p>
        <p class="narrow-paragraph">
            For avoiding this problem, etcd provides an option <span class="code-light-snippet">--strict-reconfig-check</span>. If this option is passed to etcd, etcd rejects reconfiguration requests if the number of started members will be less than
            a quorum of the reconfigured cluster. It is enabled by default.
        </p>
    </div>
</md-sidenav-layout>