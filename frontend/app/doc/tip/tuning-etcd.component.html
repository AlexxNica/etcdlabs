<md-sidenav-layout>
    <md-sidenav #sidenav [opened]="true" mode="side" class="doc-sidenav">
        <md-list dense>
            <a routerLink="/doc/{{version.etcdVersionURL}}">
                <h3 md-subheader>Getting Started ({{version.etcdVersion}})</h3>
            </a>
            <md-list-item *ngFor="let item of getStartedItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>

            <md-divider></md-divider>
            <h3 md-subheader>Operation Guides</h3>
            <md-list-item *ngFor="let item of operationItems">
                <a routerLink="{{item.url}}" class="{{item.htmlClass}}">{{item.title}}</a>
            </md-list-item>
        </md-list>
    </md-sidenav>

    <div class="doc-group">
        <router-outlet></router-outlet>

        <h4>Tuning etcd</h4>
        <p class="narrow-paragraph">
            The default settings in etcd should work well for installations on a local network where the average network latency is low. However, when using etcd across multiple data centers or over networks with high latency, the heartbeat interval and election
            timeout settings may need tuning. The network isn't the only source of latency. Each request and response may be impacted by slow disks on both the leader and follower. Each of these timeouts represents the total time from request to successful
            response from the other machine.
        </p>


        <br>
        <h5>Time Parameters</h5>
        <p class="narrow-paragraph">
            The underlying distributed consensus protocol relies on two separate time parameters to ensure that nodes can handoff leadership if one stalls or goes offline. The first parameter is called the Heartbeat Interval. This is the frequency with which the
            leader will notify followers that it is still the leader. For best practices, the parameter should be set around round-trip time between members. By default, etcd uses a <span class="code-light-snippet-red">100ms</span> heartbeat
            interval.
        </p>
        <p class="narrow-paragraph">
            The second parameter is the Election Timeout. This timeout is how long a follower node will go without hearing a heartbeat before attempting to become leader itself. By default, etcd uses a <span class="code-light-snippet-red">1000ms</span>            election timeout.
        </p>
        <p class="narrow-paragraph">
            Adjusting these values is a trade off. The value of heartbeat interval is recommended to be around the maximum of average round-trip time (RTT) between members, normally around 0.5-1.5x the round-trip time. If heartbeat interval is too low, etcd will
            send unnecessary messages that increase the usage of CPU and network resources. On the other side, a too high heartbeat interval leads to high election timeout. Higher election timeout takes longer time to detect a leader failure. The easiest
            way to measure round-trip time (RTT) is to use <a href="https://en.wikipedia.org/wiki/Ping_(networking_utility)" target="_blank" class="normal-link">PING</a> utility.
        </p>
        <p class="narrow-paragraph">
            The election timeout should be set based on the heartbeat interval and average round-trip time between members. Election timeouts must be at least 10 times the round-trip time so it can account for variance in the network. For example, if the round-trip
            time between members is <span class="code-light-snippet-red">10ms</span> then the election timeout should be at least <span class="code-light-snippet-red">100ms</span>. The election timeout should be set to at least 5 to 10 times
            the heartbeat interval to account for variance in leader replication. For a heartbeat interval of <span class="code-light-snippet-red">50ms</span>, set the election timeout to at least <span class="code-light-snippet-red">250~500ms</span>.
        </p>
        <p class="narrow-paragraph">
            The upper limit of election timeout is <span class="code-light-snippet-red">50000ms</span> (50s), which should only be used when deploying a globally-distributed etcd cluster. A reasonable round-trip time for the continental United States
            is 130ms, and the time between US and Japan is around <span class="code-light-snippet-red">350-400ms</span>. If the network has uneven performance or regular packet delays/loss then it is possible that a couple of retries may be necessary
            to successfully send a packet. So 5s is a safe upper limit of global round-trip time. As the election timeout should be an order of magnitude bigger than broadcast time, in the case of ~5s for a globally distributed cluster, then 50 seconds
            becomes a reasonable maximum.
        </p>
        <p class="narrow-paragraph">
            The heartbeat interval and election timeout value should be the same for all members in one cluster. Setting different values for etcd members may disrupt cluster stability.
        </p>
        <p class="narrow-paragraph">
            The default values can be overridden on the command line. The values are specified in milliseconds:
        </p>
        <div class="osx-window">
            <div class="window-narrow">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents"># Command line arguments:
etcd --heartbeat-interval=100 --election-timeout=500

# Environment variables:
ETCD_HEARTBEAT_INTERVAL=100
ETCD_ELECTION_TIMEOUT=500
</pre>
                </div>
            </div>
        </div>


        <br>
        <h5>Snapshot</h5>
        <p class="narrow-paragraph">
            etcd appends all key changes to a log file. This log grows forever and is a complete linear history of every change made to the keys. A complete history works well for lightly used clusters but clusters that are heavily used would carry around a large
            log. To avoid having a huge log etcd makes periodic snapshots. These snapshots provide a way for etcd to compact the log by saving the current state of the system and removing old logs.
        </p>
        <p class="narrow-paragraph">
            Creating snapshots can be expensive so they're only created after a given number of changes to etcd. By default, snapshots will be made after every <span class="code-light-snippet-red">10,000</span> changes. If etcd's memory usage and disk
            usage are too high, try lowering the snapshot threshold by setting the following on the command line:
        </p>
        <div class="osx-window">
            <div class="window-narrow">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents"># Command line arguments:
etcd --snapshot-count=5000

# Environment variables:
ETCD_SNAPSHOT_COUNT=5000
</pre>
                </div>
            </div>
        </div>


        <br>
        <h5>Network</h5>
        <p class="narrow-paragraph">
            If the etcd leader serves a large number of concurrent client requests, it may delay processing follower peer requests due to network congestion. This manifests as send buffer error messages on the follower nodes:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">dropped MsgProp to 247ae21ff9436b2d since streamMsg's sending buffer is full
dropped MsgAppResp to 247ae21ff9436b2d since streamMsg's sending buffer is full
</pre>
                </div>
            </div>
        </div>
        <p class="narrow-paragraph">
            These errors may be resolved by prioritizing etcd's peer traffic over its client traffic. On Linux, peer traffic can be prioritized by using the traffic control mechanism:
        </p>
        <div class="osx-window">
            <div class="window">
                <div class="titlebar">
                    <div class="buttons">
                        <div class="closebtn"><span><strong></strong></span></div>
                        <div class="minimize"><span><strong></strong></span></div>
                        <div class="zoom"><span><strong></strong></span></div>
                    </div><span class="title-bar-text">terminal</span></div>
                <div class="content">
                    <pre class="osx-terminal-contents">tc qdisc add dev eth0 root handle 1: prio bands 3
tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1
tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1
tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2739 0xffff flowid 1:1
tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2739 0xffff flowid 1:1
</pre>
                </div>
            </div>
        </div>

    </div>
</md-sidenav-layout>